---
title: "Intro to Machine Learning"
date: "October 25, 2015"
output:  html_document
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
```
```{r}
rm( list = ls())  # Clear environment
```

# Lab 6.4 - Regression Trees

````{r}
# Load required libraries

# install.packages("ElemStatLearn")
# install.packages("partykit")
# install.packages("randomForest")

library(rpart)          #classification and regression trees
library(partykit)       #treeplots
library(MASS)           #breast and pima indian data
library(ElemStatLearn)  #prostate data
library(randomForest)   #random forests
library(gbm)            #gradient boosting
library(caret)          #tune hyper-parameters
````

***
## 1\. Using Regression Trees

#### a\. Load sample data: (Prostate) 

````{r}
data(prostate)
prostate$gleason = ifelse(prostate$gleason == 6, 0, 1)
pros.train = subset(prostate, train==TRUE)[,1:9]
pros.test = subset(prostate, train==FALSE)[,1:9]
````

#### b\. use the rpart() funtion to create a regression tree on the train data

````{r}
tree.pros = rpart(lpsa~., data=pros.train)
print(tree.pros$cptable)
````

* **CP** is the cost complexity parameter  
* **nsplit** is the number of splits in the tree

````{r}
plotcp(tree.pros)
````

#### c\. Minimize xerror by pruning & compare

````{r echo=TRUE}
cp = min(tree.pros$cptable[5,])
prune.tree.pros = prune(tree.pros, cp = cp)
````

````{r echo=TRUE}
plot(as.party(tree.pros))  # Full tree
plot(as.party(prune.tree.pros))  # Pruned tree
````


#### d\. Evalute model on Test data

````{r}
party.pros.test = predict(prune.tree.pros, newdata=pros.test)
rpart.resid = party.pros.test - pros.test$lpsa #calculate residuals
mean(rpart.resid^2) #caluclate MSE
````

***
## 2\. Using Classification Trees

#### a\. Load data set:

````{r}
data(biopsy)
biopsy = biopsy[,-1] #delete ID
names(biopsy) = c("thick", "u.size", "u.shape", "adhsn", "s.size", "nucl", "chrom", "n.nuc", "mit", "class") #change the feature names
biopsy.v2 = na.omit(biopsy) #delete the observations with missing values

set.seed(123) #random number generator
ind = sample(2, nrow(biopsy.v2), replace=TRUE, prob=c(0.7, 0.3))

biop.train = biopsy.v2[ind==1,] #training data set
biop.test = biopsy.v2[ind==2,]  #test data set
````

#### b\. Review test data:

````{r}
str(biop.test[,10])
````

#### c\. Create classification model using rpart

````{r}
set.seed(123)
tree.biop = rpart(class~., data=biop.train)
print(tree.biop$cptable)
````

#### d\. Prune tree and compare plots

````{r}
cp = min(tree.biop$cptable[3,])
prune.tree.biop = prune(tree.biop, cp = cp)
````


````{r}
plot(as.party(tree.biop))
plot(as.party(prune.tree.biop))
````

#### e\. Evaluate model w/ MSE

````{r}
rparty.test = predict(prune.tree.biop, newdata=biop.test, type="class")
table(rparty.test, biop.test$class)
````

Prediction accuracy: `r (136+64)/209`

***
## 3\. Random forest regression

#### a\. Create random forest model

````{r}
set.seed(123)
rf.pros = randomForest(lpsa~., data=pros.train)
print(rf.pros)
````

````{r}
plot(rf.pros)
````

#### b\.Determine number of trees for minimum error and standard error

````{r echo=TRUE}
which.min(rf.pros$mse)
````

#### c\. Set ntree=70 and create new model

````{r echo=TRUE}
set.seed(123)
rf.pros.2 = randomForest(lpsa~., data=pros.train, ntree=70)
print(rf.pros.2)
````


#### d\. Evaluate model features -- feature weight  

````{r}
varImpPlot(rf.pros.2, main="Variable Importance Plot - PSA Score")
````

#### Review feature weight using importance()

````{r echo-TRUE}
importance(rf.pros.2)
````

***
## 4\. Using Random Forest Classification

#### a\. sample data

````{r echo=TRUE}
set.seed(123)
rf.biop = randomForest(class~., data=biop.train)
print(rf.biop)
````

#### b\. Plot error by trees

````{r}
plot(rf.biop)
````

#### c.\ Optimize model accuracy

````{r echo=TRUE}
which.min(rf.biop$err.rate[,1])
````

#### d.\ Run model with tree parameter

````{r}
rf.biop.2 = randomForest(class~., data=biop.train, ntree=19)
print(rf.biop.2)
````

#### e\. Evaluate results

````{r}
rf.biop.test = predict(rf.biop.2, newdata=biop.test, type="response")
table(rf.biop.test, biop.test$class)
````

Prediction accuracy: `r (139+67)/209`

#### Feature mapping (Gini Index)
````{r}
varImpPlot(rf.biop.2)
````

***
## 5\. Random Forest Classification on PIMA sample

#### a\. Load sample data

````{r}
data(Pima.tr)
data(Pima.te)
pima = rbind(Pima.tr, Pima.te)

set.seed(502)
nd = sample(2, nrow(pima), replace=TRUE, prob=c(0.7,0.3))

pima.train = pima[ind==1,]
pima.test  = pima[ind==2,]
````

#### b\. Create classification random forest

````{r}
set.seed(321)
rf.pima = randomForest(type~., data=na.omit(pima.train))
print(rf.pima)
````

#### c\. Optmize Model from Training data

````{r echo=TRUE}
which.min(rf.pima$err.rate[,1])
````

````{r}
set.seed(321)
rf.pima.2 = randomForest(type~., data=na.omit(pima.train), ntree=80)
print(rf.pima.2)
````

#### d\. Evaluate model

````{r}
set.seed(321)
rf.pima.2 = randomForest(type~., data=na.omit(pima.train), ntree=80)
print(rf.pima.2)
````

#### **End**
