---
title: "Intro to Machine Learning"
date: "October 25, 2015"
output:  html_document
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
```
```{r}
rm( list = ls())  # Clear environment
```

# Lab 6.2 - Logistic Regression

***
## 1\. Identify whether a tumor biopsy was malignant or benign.

a\. Load sample data:

````{r}
library(MASS)
data(biopsy)

#Data Cleanup
biopsy$ID = NULL
names(biopsy) = c("thick", "u.size", "u.shape", "adhsn", "s.size", "nucl", "chrom", "n.nuc", "mit", "class")

#New data sample with NA removed
biopsy.v2 = na.omit(biopsy)
head(biopsy.v2,5)
````

b\. **Exploratory Analysis**: Create a box plot matrix to understanding which features may be important to the algorithm.

#### Plot features

````{r}
library(reshape2)
library(ggplot2)

# Create new data frame from MELT
biop.m = melt(biopsy.v2, id.var="class")
ggplot(data=biop.m, aes(x=class, y=value)) + geom_boxplot() +facet_wrap(~variable,ncol = 3)
````

#### Correlation analysis

````{r}
library(corrplot)
bc = cor(biopsy.v2[ ,1:9]) #create an object of the features
corrplot.mixed(bc)
````

c\. Separate data into Test / Training sets

````{r}
set.seed(123) #random number generator
ind = sample(2, nrow(biopsy.v2), replace=TRUE, prob=c(0.7, 0.3))

train = biopsy.v2[ind==1,] #the training data set
test  = biopsy.v2[ind==2,] #the test data set
str(test) #confirm it worked
````

#### Confirm the distribution in the two data sets:

**Test Set**
````{r}
table(train$class)
````  

**Training Set**
````{r}
table(test$class)
````

d\. Logistic regression modeling

#### Full fit
````{r}
full.fit = glm(class~., family=binomial, data=train)
summary(full.fit)
````

#### Review confidence interval

````{r}
confint(full.fit)
````

#### Produce odds ratios

````{r}
exp(coef(full.fit))
````


#### Review collinearity 

````{r}
library(car)
vif(full.fit)
````

#### Feature Selection

Inspect the first 5 predicted probabilities

````{r}
train$probs = predict(full.fit, type="response")
train$probs[1:5]
````

Confirm binary results

````{r}
contrasts(train$class)
````

#### Evaluate training model results using confusion matrix

````{r}
train$predict = rep("benign", 474)
train$predict[train$probs>0.5]="malignant"
table(train$predict, train$class)
````

Percentage predicted correctly: `r mean(train$predict==train$class)`

#### Evalutate test model results

````{r}
test$prob = predict(full.fit, newdata=test, type="response")

test$predict = rep("benign", 209)
test$predict[test$prob>0.5]="malignant"

table(test$predict, test$class)
````

Percentage predicted correctly: `r mean(test$predict==test$class)`

***
## 2\. Logistic regression with cross-validation

a\. Add a vector to the train set, code it all with zeroes, and then code it to one where the class vector is equal to malignant, as follows:

````{r}
# install.packages("bestglm")
library(bestglm)

train$y=rep(0,474)
train$y[train$class=="malignant"]=1

head(train[ ,13],5)
````

b\. Remove unneeded features

````{r}
biopsy.cv = train[ ,-10:-12]
head(biopsy.cv)
````

c\. Fit model using **bestglm**

````{r}
bestglm(Xy = biopsy.cv, IC="CV", CVArgs=list(Method="HTF", K=10, REP=1), family=binomial)
````

d\. Fit reduced model using **glm**

````{r}
reduce.fit = glm(class~thick+u.size+nucl, family=binomial, data=train)

train$cv.probs = predict(reduce.fit, type="response")
train$cv.predict = rep("benign", 474)
train$cv.predict[train$cv.probs>0.5]="malignant"
table(train$cv.predict, train$class)
````

e\. Evaluate model test data results

````{r}
test$cv.probs = predict(reduce.fit, newdata=test, type="response")
test$predict = rep("benign", 209)
test$predict[test$cv.probs>0.5]="malignant"
table(test$predict, test$class)
````

f\. 2nd attempt to fit using "BIC" algorithym

````{r}
bestglm(Xy= biopsy.cv, IC="BIC", family=binomial)
````

g\. Evaluate model with test data set

````{r}
bic.fit=glm(class~thick+adhsn+nucl+n.nuc, family=binomial, data=train)
test$bic.probs = predict(bic.fit, newdata=test, type="response")
test$bic.predict = rep("benign", 209)
test$bic.predict[test$bic.probs>0.5]="malignant"
table(test$bic.predict, test$class)
````

***
## 3\. Linear Discriminant Analysis (LDA)

a\. Create new data frames for training and test data sets

````{r echo=TRUE}
# Training data
lda.train = train[ ,-11:-15]
lda.train[1:3,]
# Testing data
lda.test = test[ ,-11:-15]
lda.test[1:3,]
````

b\. Create new LDA model

````{r}
lda.fit = lda(class~., data=lda.train)
lda.fit
````

####Plot results

````{r}
plot(lda.fit, type="both")
````

c\. Evaluate model on Training Set

````{r}
lda.predict = predict(lda.fit)
train$lda = lda.predict$class
table(train$lda, train$class)
````

d\. Evaluate model on Test Set

````{r}
lda.test = predict(lda.fit, newdata = test)
test$lda = lda.test$class
table(test$lda, test$class)
````

Percentage predicted correctly: `r mean(test$lda==test$class)`


#### **End**
